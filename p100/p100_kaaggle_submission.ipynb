{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport git\nimport os\n\ndirectory = \"abhi\"\nparent_dir = \"/kaggle/working\"\n\ntry:\n    BASE_PATH = os.path.join(parent_dir, directory)  \n    os.mkdir(BASE_PATH) \n    print(\"Directory '%s' created\" %directory) \nexcept:\n    pass\n\ngit.Git(BASE_PATH).clone(\"https://github.com/abhisha1991/w251_hw6\")\nprint(\"Repository is created\")","execution_count":1,"outputs":[{"output_type":"stream","text":"Directory 'abhi' created\nRepository is created\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# steps taken from here: https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-python\n\n!pip install ibm-cos-sdk\nimport boto3\nfrom botocore.client import Config\nimport ibm_boto3\nfrom ibm_botocore.client import Config, ClientError\n\n# Constants for IBM COS values - BAD PRACTICE TO EXPOSE THE VALUES\nCOS_ENDPOINT = \"https://s3.eu-de.cloud-object-storage.appdomain.cloud\"\nCOS_API_KEY_ID = \"<<REDACTED>>\"\nCOS_SERVICE_CRN = \"crn:v1:bluemix:public:iam-identity::a/f3de9bdab4ee4727b5f81c87efa6d530::serviceid:ServiceId-2ef85b26-fc98-4444-bc26-2af926723bfa\"\nCOS_AUTH_ENDPOINT = \"https://iam.bluemix.net/oidc/token\"\nCOS_RESOURCE_CRN = \"crn:v1:bluemix:public:cloud-object-storage:global:a/f3de9bdab4ee4727b5f81c87efa6d530:1f545981-7f91-47a0-8437-6b4c581e31e2::\"\nBUCKET_NAME = \"abhihw6bucket\"\n\n# Create resource\ncos = ibm_boto3.resource(\"s3\",\n    ibm_api_key_id=COS_API_KEY_ID,\n    ibm_service_instance_id=COS_RESOURCE_CRN,\n    ibm_auth_endpoint=COS_AUTH_ENDPOINT,\n    config=Config(signature_version=\"oauth\"),\n    endpoint_url=COS_ENDPOINT\n)\n\n'''\n# Create client \ncos = ibm_boto3.client(\"s3\",\n    ibm_api_key_id=COS_API_KEY_ID,\n    ibm_service_instance_id=COS_SERVICE_CRN,\n    config=Config(signature_version=\"oauth\"),\n    endpoint_url=COS_ENDPOINT\n)\n'''\n\ndef get_bucket_item_list(bucket_name):\n    print(\"Retrieving bucket contents from: {0}\".format(bucket_name))\n    try:\n        files = cos.Bucket(bucket_name).objects.all()\n        for file in files:\n            print(\"Item: {0} ({1} bytes).\".format(file.key, file.size))\n    except ClientError as be:\n        print(\"CLIENT ERROR: {0}\\n\".format(be))\n    except Exception as e:\n        print(\"Unable to retrieve bucket contents: {0}\".format(e))\n    return files\n\ndef get_item(bucket_name, item_name):\n    print(\"Retrieving item from bucket: {0}, key: {1}\".format(bucket_name, item_name))\n    try:\n        file = cos.Object(bucket_name, item_name).get()\n        # print(\"File Contents: {0}\".format(file[\"Body\"].read()))\n    except ClientError as be:\n        print(\"CLIENT ERROR: {0}\\n\".format(be))\n    except Exception as e:\n        print(\"Unable to retrieve file contents: {0}\".format(e))\n    return file[\"Body\"].read()\n\nall_files = get_bucket_item_list(BUCKET_NAME)\nfor file in all_files:\n    if file.size > 0:\n        file_content = get_item(BUCKET_NAME, file.key)\n        # Download this file locally\n        d = BASE_PATH + \"/\" + '/'.join(file.key.split('/')[0:-1])\n        if not os.path.exists(d):\n            os.makedirs(d)\n        f = open(BASE_PATH + \"/\" + file.key, \"wb\")\n        f.write(file_content)\n        f.close()","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting ibm-cos-sdk\n  Downloading ibm-cos-sdk-2.6.3.tar.gz (54 kB)\n\u001b[K     |████████████████████████████████| 54 kB 1.3 MB/s eta 0:00:011\n\u001b[?25hCollecting ibm-cos-sdk-core==2.6.3\n  Downloading ibm-cos-sdk-core-2.6.3.tar.gz (823 kB)\n\u001b[K     |████████████████████████████████| 823 kB 8.4 MB/s eta 0:00:01\n\u001b[?25hCollecting ibm-cos-sdk-s3transfer==2.6.3\n  Downloading ibm-cos-sdk-s3transfer-2.6.3.tar.gz (218 kB)\n\u001b[K     |████████████████████████████████| 218 kB 12.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from ibm-cos-sdk) (0.10.0)\nRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.7/site-packages (from ibm-cos-sdk-core==2.6.3->ibm-cos-sdk) (0.15.2)\nRequirement already satisfied: requests<3.0,>=2.18 in /opt/conda/lib/python3.7/site-packages (from ibm-cos-sdk-core==2.6.3->ibm-cos-sdk) (2.23.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from ibm-cos-sdk-core==2.6.3->ibm-cos-sdk) (2.8.1)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.18->ibm-cos-sdk-core==2.6.3->ibm-cos-sdk) (2.9)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.18->ibm-cos-sdk-core==2.6.3->ibm-cos-sdk) (1.24.3)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.18->ibm-cos-sdk-core==2.6.3->ibm-cos-sdk) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.18->ibm-cos-sdk-core==2.6.3->ibm-cos-sdk) (2020.4.5.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->ibm-cos-sdk-core==2.6.3->ibm-cos-sdk) (1.14.0)\nBuilding wheels for collected packages: ibm-cos-sdk, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer\n  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ibm-cos-sdk: filename=ibm_cos_sdk-2.6.3-py2.py3-none-any.whl size=72509 sha256=3a1e6a91c2bb7ad236a31b469242714815dfbe12cd5d7a5c4c96031dfe7680b8\n  Stored in directory: /root/.cache/pip/wheels/95/08/0c/5e9dbe3873d3aaa1e3a547d00be5a40d0aed5f22b29b36e275\n  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ibm-cos-sdk-core: filename=ibm_cos_sdk_core-2.6.3-py2.py3-none-any.whl size=500835 sha256=33eb216bd5bd92a64f2db2bec1c565039dba7022ffbce6b11a7e00198b0d92c2\n  Stored in directory: /root/.cache/pip/wheels/c0/18/74/b3634906c1fe3e2c82fb61ba02a6462414828cbb693354d056\n  Building wheel for ibm-cos-sdk-s3transfer (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ibm-cos-sdk-s3transfer: filename=ibm_cos_sdk_s3transfer-2.6.3-py2.py3-none-any.whl size=88602 sha256=9033748b5f1a5ed1ab91b60d16256e5a0d5f14dcb19b8e01cb6b2f6deefaf453\n  Stored in directory: /root/.cache/pip/wheels/64/96/f4/8ec4e09fe9eaeebb3247058788d6512212571d7529af7fe7ef\nSuccessfully built ibm-cos-sdk ibm-cos-sdk-core ibm-cos-sdk-s3transfer\nInstalling collected packages: ibm-cos-sdk-core, ibm-cos-sdk-s3transfer, ibm-cos-sdk\nSuccessfully installed ibm-cos-sdk-2.6.3 ibm-cos-sdk-core-2.6.3 ibm-cos-sdk-s3transfer-2.6.3\nRetrieving bucket contents from: abhihw6bucket\nItem: vm_output/ (0 bytes).\nItem: vm_output/.ipynb_checkpoints/ (0 bytes).\nItem: vm_output/.ipynb_checkpoints/BERT_classifying_toxicity-checkpoint.ipynb (64617 bytes).\nItem: vm_output/BERT_classifying_toxicity.ipynb (64617 bytes).\nItem: vm_output/PYTORCH.build (950 bytes).\nItem: vm_output/README.md (5597 bytes).\nItem: vm_output/bert_pytorch.bin (437982311 bytes).\nItem: vm_output/data/ (0 bytes).\nItem: vm_output/data/cased_L-12_H-768_A-12/ (0 bytes).\nItem: vm_output/data/cased_L-12_H-768_A-12/bert_config.json (313 bytes).\nItem: vm_output/data/cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001 (435731736 bytes).\nItem: vm_output/data/cased_L-12_H-768_A-12/bert_model.ckpt.index (8524 bytes).\nItem: vm_output/data/cased_L-12_H-768_A-12/bert_model.ckpt.meta (905891 bytes).\nItem: vm_output/data/cased_L-12_H-768_A-12/vocab.txt (213450 bytes).\nItem: vm_output/data/download.sh (632 bytes).\nItem: vm_output/data/test.csv (30179878 bytes).\nItem: vm_output/data/train.csv (816211476 bytes).\nItem: vm_output/data/uncased_L-12_H-768_A-12/ (0 bytes).\nItem: vm_output/data/uncased_L-12_H-768_A-12/bert_config.json (313 bytes).\nItem: vm_output/data/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001 (440425712 bytes).\nItem: vm_output/data/uncased_L-12_H-768_A-12/bert_model.ckpt.index (8528 bytes).\nItem: vm_output/data/uncased_L-12_H-768_A-12/bert_model.ckpt.meta (904243 bytes).\nItem: vm_output/data/uncased_L-12_H-768_A-12/vocab.txt (231508 bytes).\nItem: vm_output/vmo/ (0 bytes).\nItem: vm_output/workingdir/ (0 bytes).\nItem: vm_output/workingdir/bert_config.json (313 bytes).\nItem: vm_output/workingdir/pytorch_model.bin (440474402 bytes).\nItem: vm_output/workingdir/tmp (0 bytes).\nRetrieving item from bucket: abhihw6bucket, key: vm_output/.ipynb_checkpoints/BERT_classifying_toxicity-checkpoint.ipynb\nRetrieving item from bucket: abhihw6bucket, key: vm_output/BERT_classifying_toxicity.ipynb\nRetrieving item from bucket: abhihw6bucket, key: vm_output/PYTORCH.build\nRetrieving item from bucket: abhihw6bucket, key: vm_output/README.md\nRetrieving item from bucket: abhihw6bucket, key: vm_output/bert_pytorch.bin\nRetrieving item from bucket: abhihw6bucket, key: vm_output/data/cased_L-12_H-768_A-12/bert_config.json\nRetrieving item from bucket: abhihw6bucket, key: vm_output/data/cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001\nRetrieving item from bucket: abhihw6bucket, key: vm_output/data/cased_L-12_H-768_A-12/bert_model.ckpt.index\nRetrieving item from bucket: abhihw6bucket, key: vm_output/data/cased_L-12_H-768_A-12/bert_model.ckpt.meta\nRetrieving item from bucket: abhihw6bucket, key: vm_output/data/cased_L-12_H-768_A-12/vocab.txt\nRetrieving item from bucket: abhihw6bucket, key: vm_output/data/download.sh\nRetrieving item from bucket: abhihw6bucket, key: vm_output/data/test.csv\nRetrieving item from bucket: abhihw6bucket, key: vm_output/data/train.csv\nRetrieving item from bucket: abhihw6bucket, key: vm_output/data/uncased_L-12_H-768_A-12/bert_config.json\nRetrieving item from bucket: abhihw6bucket, key: vm_output/data/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001\nRetrieving item from bucket: abhihw6bucket, key: vm_output/data/uncased_L-12_H-768_A-12/bert_model.ckpt.index\nRetrieving item from bucket: abhihw6bucket, key: vm_output/data/uncased_L-12_H-768_A-12/bert_model.ckpt.meta\nRetrieving item from bucket: abhihw6bucket, key: vm_output/data/uncased_L-12_H-768_A-12/vocab.txt\nRetrieving item from bucket: abhihw6bucket, key: vm_output/workingdir/bert_config.json\nRetrieving item from bucket: abhihw6bucket, key: vm_output/workingdir/pytorch_model.bin\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# taking the rest from here - we need to load the model and run it\n# https://www.kaggle.com/abhishek/pytorch-bert-inference\n\nimport sys\npackage_dir = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.append(package_dir)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch.utils.data\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\nimport warnings\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\nfrom pytorch_pretrained_bert import BertConfig\n\nwarnings.filterwarnings(action='once')\n# we have to comment out the below because the notebook is running on kaggle's server which unfortunately doesnt allow for cuda training in its free version\n# device = torch.device('cuda')\n# instead we define the device as CPU based\ndevice = torch.device('cpu')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 220\nSEED = 1234\nBATCH_SIZE = 32\nBERT_MODEL_PATH = BASE_PATH + \"/\" + \"vm_output/data/uncased_L-12_H-768_A-12\"\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\nbert_config = BertConfig(BERT_MODEL_PATH + \"/bert_config.json\")\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(BASE_PATH + \"/vm_output/data/test.csv\")\ntest_df['comment_text'] = test_df['comment_text'].astype(str) \n\n# Unfortunately, since we are evaluating on a cpu based backend and cuda is disabled, doing inference for 3000+ test batches (when batch size = 32)\n# is going to take over 10 hours! Instead, we consider only the top n rows which get processed in batches of 32\ntest_df = test_df[:128]\n\n\nX_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)","execution_count":7,"outputs":[{"output_type":"stream","text":"100%|██████████| 128/128 [00:00<00:00, 489.16it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BertForSequenceClassification(bert_config, num_labels=1)\nmodel.load_state_dict(torch.load(BASE_PATH + \"/vm_output/bert_pytorch.bin\", map_location=device))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): BertLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=1, bias=True)\n)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest_preds = np.zeros((len(X_test)))\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n    test_preds[i * 32:(i + 1) * 32] = pred[:, 0].detach().cpu().squeeze().numpy()\n\ntest_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()","execution_count":9,"outputs":[{"output_type":"stream","text":"100%|██████████| 4/4 [00:58<00:00, 14.61s/it]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': test_pred\n})\nsubmission.to_csv('submission.csv', index=False)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[10:30]","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"         id  prediction\n10  7000010    0.000635\n11  7000011    0.043246\n12  7000012    0.000571\n13  7000013    0.000101\n14  7000014    0.000393\n15  7000015    0.000367\n16  7000016    0.042951\n17  7000017    0.000167\n18  7000018    0.340283\n19  7000019    0.014809\n20  7000020    0.000121\n21  7000021    0.000098\n22  7000022    0.000118\n23  7000023    0.441729\n24  7000024    0.936073\n25  7000025    0.000238\n26  7000026    0.092070\n27  7000027    0.000095\n28  7000028    0.000179\n29  7000029    0.000914","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>7000010</td>\n      <td>0.000635</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>7000011</td>\n      <td>0.043246</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>7000012</td>\n      <td>0.000571</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>7000013</td>\n      <td>0.000101</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>7000014</td>\n      <td>0.000393</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>7000015</td>\n      <td>0.000367</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>7000016</td>\n      <td>0.042951</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>7000017</td>\n      <td>0.000167</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>7000018</td>\n      <td>0.340283</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>7000019</td>\n      <td>0.014809</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>7000020</td>\n      <td>0.000121</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>7000021</td>\n      <td>0.000098</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>7000022</td>\n      <td>0.000118</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>7000023</td>\n      <td>0.441729</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>7000024</td>\n      <td>0.936073</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>7000025</td>\n      <td>0.000238</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>7000026</td>\n      <td>0.092070</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>7000027</td>\n      <td>0.000095</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>7000028</td>\n      <td>0.000179</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>7000029</td>\n      <td>0.000914</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}